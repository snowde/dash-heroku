{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst & Best Noun and Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: free: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Library Packages \n",
    "import regex as re\n",
    "import itertools as it\n",
    "import spacy\n",
    "\n",
    "%run libraries.py \n",
    "from __future__ import division\n",
    "\n",
    "# Settings \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def front(self, n):\n",
    "    return self.iloc[:, :n]\n",
    "\n",
    "def back(self, n):\n",
    "    return self.iloc[:, -n:]\n",
    "\n",
    "# Like normalization, standardization can be useful, and even required in some\n",
    "# machine learning algorithms when your time series data has input values with differing scales.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def Standardisation(df):\n",
    "    listed = list(df)\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(scaled)\n",
    "    df.columns = listed\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "!free -h\n",
    "\n",
    "yelp = pd.read_csv(\"bjs-restaurant-sanbruno.csv\")\n",
    "\n",
    "yelp[\"date\"] = yelp[\"date\"].apply(lambda x: x[:10])\n",
    "yelp[\"date\"] = yelp[\"date\"].apply(lambda x: x[:-1] if x[-1]==\"\\\\\" else x )\n",
    "yelp[\"date\"] = yelp[\"date\"].apply(lambda x: x[:-2] if x[-1]==\"n\" else x )\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "yelp['date'] = yelp['date'].apply(lambda x: parse(x))\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "yelp[\"positive\"] = 0\n",
    "yelp[\"compound\"] =  0.0\n",
    "yelp[\"negative\"] = 0\n",
    "yelp[\"neutral\"] =  0 \n",
    "\n",
    "analyzer = SIA()\n",
    "for sentence, row in zip(yelp[\"review\"], list(range(yelp.shape[0]))):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    yelp[\"compound\"][row] = float(vs[\"compound\"])\n",
    "    if vs[\"compound\"] <-0.5:\n",
    "        yelp[\"negative\"][row] = 1\n",
    "    elif vs[\"compound\"] >0.5:\n",
    "        yelp[\"positive\"][row] = 1\n",
    "    else:\n",
    "        yelp[\"neutral\"][row] = 1\n",
    "    #print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "\n",
    "worst = yelp[(yelp[\"rating\"]==1) & (yelp[\"compound\"]<-.95) ]\n",
    "worst = worst.sort_values(\"date\", ascending=False).head(10).reset_index()\n",
    "\n",
    "best = yelp[(yelp[\"rating\"]==5) & (yelp[\"compound\"]>.95) ]\n",
    "best = best.sort_values(\"date\", ascending=False).head(10).reset_index()\n",
    "\n",
    "# Entity Extraction From Review\n",
    "import itertools as it\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "sample_review = \"\" \n",
    "for i in best[\"review\"]:\n",
    "    sample_review = sample_review + str(i)\n",
    "\n",
    "#print(sample_review)\n",
    "\n",
    "len(sample_review)\n",
    "\n",
    "sample_review = sample_review.replace(\"\\\\\", \"\")\n",
    "\n",
    "parsed_review = nlp(sample_review)\n",
    "\n",
    "#print(parsed_review)\n",
    "\n",
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame({'token_text':token_text, 'part_of_speech':token_pos})\n",
    "\n",
    "# Unigrams\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "token = nltk.word_tokenize(str(parsed_review))\n",
    "grams = ngrams(token,1)\n",
    "\n",
    "dra = Counter(grams)\n",
    "\n",
    "t = pd.DataFrame()\n",
    "f = pd.DataFrame(list(dra.keys()))\n",
    "\n",
    "f = f[0]\n",
    "\n",
    "t[\"name\"] = f \n",
    "t[\"count\"] = list(dra.values())\n",
    "\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "r = pd.merge(t, df, left_on=[\"name\"], right_on=[\"token_text\"], how=\"left\",right_index=False)\n",
    "r = r.drop(\"token_text\",axis=1)\n",
    "r.columns = [\"name\",\"count\",\"pos\"]\n",
    "\n",
    "# Entity Extraction From Review\n",
    "import itertools as it\n",
    "import spacy\n",
    "\n",
    "def g_o_b(type_df):\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    sample_review = \"\" \n",
    "    for i in type_df[\"review\"]:\n",
    "        sample_review = sample_review + str(i)\n",
    "\n",
    "    #print(sample_review)\n",
    "\n",
    "    len(sample_review)\n",
    "\n",
    "    sample_review = sample_review.replace(\"\\\\\", \"\")\n",
    "\n",
    "    parsed_review = nlp(sample_review)\n",
    "\n",
    "    #print(parsed_review)\n",
    "\n",
    "    token_text = [token.orth_ for token in parsed_review]\n",
    "    token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "    df = pd.DataFrame({'token_text':token_text, 'part_of_speech':token_pos})\n",
    "\n",
    "    # Unigrams\n",
    "    import nltk\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.util import ngrams\n",
    "    from collections import Counter\n",
    "\n",
    "    token = nltk.word_tokenize(str(parsed_review))\n",
    "    grams = ngrams(token,1)\n",
    "\n",
    "    dra = Counter(grams)\n",
    "\n",
    "    t = pd.DataFrame()\n",
    "    f = pd.DataFrame(list(dra.keys()))\n",
    "\n",
    "    f = f[0]\n",
    "\n",
    "    t[\"name\"] = f \n",
    "    t[\"count\"] = list(dra.values())\n",
    "\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    r = pd.merge(t, df, left_on=[\"name\"], right_on=[\"token_text\"], how=\"left\",right_index=False)\n",
    "    r = r.drop(\"token_text\",axis=1)\n",
    "    r.columns = [\"name\",\"count\",\"pos\"]\n",
    "\n",
    "    dfs = r[r[\"pos\"]==\"NOUN\"].sort_values(\"count\",ascending=False)\n",
    "    return dfs\n",
    "\n",
    "def firstme(first, second, tex1,tex2):\n",
    "    ras = g_o_b(first).head(10)\n",
    "    vas = g_o_b(second)\n",
    "\n",
    "    vas = vas[vas.name.isin(list(ras[\"name\"].values))].sort_values(\"count\",ascending=False).head(10)\n",
    "\n",
    "    kas = pd.merge(ras, vas, on=\"name\",how=\"left\")\n",
    "\n",
    "    kas.fillna(value=0, inplace=True)\n",
    "    kas.columns = [\"name\",tex1,\"da\",tex2,\"da2\"]\n",
    "    kas.drop([\"da\",\"da2\"],axis=1)\n",
    "    return kas\n",
    "\n",
    "def gobp(type_df):\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    sample_review = \"\" \n",
    "    for i in type_df[\"review\"]:\n",
    "        sample_review = sample_review + str(i)\n",
    "\n",
    "    #print(sample_review)\n",
    "\n",
    "    len(sample_review)\n",
    "\n",
    "    sample_review = sample_review.replace(\"\\\\\", \"\")\n",
    "\n",
    "    parsed_review = nlp(sample_review)\n",
    "\n",
    "    ent = []\n",
    "    lab = []\n",
    "\n",
    "\n",
    "    for num, entity in enumerate(parsed_review.ents):\n",
    "        ent.append(entity[0])\n",
    "        lab.append(entity.label_)\n",
    "\n",
    "    ent_df = pd.DataFrame()\n",
    "    ent_df[\"entity\"] = ent\n",
    "    ent_df[\"label\"] = lab\n",
    "    rab = ent_df\n",
    "    # for num, entity in enumerate(parsed_review.ents):\n",
    "    #     ent_df[\"entity\"][num] = entity\n",
    "    #     ent_df[\"label\"][num] = entity.label_\n",
    "\n",
    "\n",
    "    ent_df[\"entity\"] = ent_df[\"entity\"].astype(str)\n",
    "\n",
    "    ent_df = pd.merge(ent_df.groupby(\"entity\").count().reset_index(), ent_df.drop_duplicates(\"entity\"), on=\"entity\",how=\"left\")\n",
    "\n",
    "    ent_df.columns = [\"entity\",\"count\",\"type\"]\n",
    "\n",
    "    from difflib import SequenceMatcher\n",
    "\n",
    "    def similar(a, b):\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "    vent = ent_df[ent_df[\"type\"].isin([\"GPE\",\"PERSON\",\"ORG\"])][\"entity\"]\n",
    "\n",
    "    import jellyfish\n",
    "    from fuzzywuzzy import fuzz\n",
    "    from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "    dar = []\n",
    "    sar = []\n",
    "    kar = []\n",
    "    jar = []\n",
    "    lev = []\n",
    "\n",
    "\n",
    "    for i in vent:\n",
    "        for r in vent:\n",
    "            dar.append(i)\n",
    "            sar.append(r)\n",
    "            jar.append(jellyfish.jaro_distance(i,r))\n",
    "            kar.append(similar(i,r))\n",
    "            lev.append(jellyfish.levenshtein_distance(i,r))\n",
    "\n",
    "    sos = pd.DataFrame()\n",
    "    sos[\"original\"] = dar\n",
    "    sos[\"match\"] = sar\n",
    "    sos[\"percentage\"] = kar\n",
    "    sos[\"distance\"] = jar\n",
    "    sos[\"leven\"] = lev\n",
    "    sos[\"together\"] = (sos[\"percentage\"]  + (sos[\"distance\"])/2)*(1/sos[\"leven\"])\n",
    "                    # Including leven is important because it also counts the number\n",
    "                    # of characters, maybe change below, 0.2 to 0.3 if further issues. \n",
    "\n",
    "    sos = sos[(sos[\"together\"]<1.0)&(sos[\"together\"]>0.4)].reset_index()\n",
    "\n",
    "    sos[\"count_original\"] = 0 \n",
    "    sos[\"count_contender\"] = 0 \n",
    "    for i, c, r in zip(sos[\"original\"], sos[\"match\"], list(range(sos.shape[0]))):\n",
    "        da = np.where(ent_df[\"entity\"]==i, ent_df[\"count\"],np.nan )\n",
    "        x = da[~np.isnan(da)]\n",
    "        sos[\"count_original\"][r] = x\n",
    "\n",
    "        da = np.where(ent_df[\"entity\"]==c, ent_df[\"count\"],np.nan )\n",
    "        x = da[~np.isnan(da)]\n",
    "        sos[\"count_contender\"][r] = x\n",
    "\n",
    "\n",
    "    sos\n",
    "\n",
    "    dar = np.where(sos[\"count_original\"]>=sos[\"count_contender\"],sos[\"original\"],sos[\"match\"])\n",
    "    sos[\"final\"] = dar\n",
    "\n",
    "    cas = sos[[\"match\",\"final\"]]\n",
    "\n",
    "\n",
    "    for match, final in zip(cas[\"match\"],cas[\"final\"]):\n",
    "        print(match)\n",
    "        ent_df['entity'] = ent_df.entity.replace([str(match)],[str(final)])\n",
    "\n",
    "    res = pd.DataFrame()\n",
    "    res[\"start\"] = sos[\"original\"]\n",
    "\n",
    "    ent_df = pd.merge(ent_df.groupby(\"entity\").sum().reset_index(), ent_df.sort_values([\"entity\",\"count\"],ascending=[\"False\",\"False\"]).drop_duplicates(\"entity\",keep=\"first\"), on=\"entity\",how=\"left\")\n",
    "\n",
    "    ent_df[\"count\"] = ent_df[\"count_x\"]\n",
    "\n",
    "    ent_df = ent_df[[\"entity\",\"count\",\"type\"]].sort_values(\"count\",ascending=False)\n",
    "\n",
    "    ent_df = ent_df[ent_df[\"type\"].isin([\"ORG\",\"PERSON\",\"GPE\"])]\n",
    "    # If a person uses the word twice, there is probably a good reasons, so done in that way. \n",
    "    return ent_df\n",
    "\n",
    "def firstme(first, second, tex1,tex2):\n",
    "    ras = g_o_b(first).head(10)\n",
    "    vas = g_o_b(second)\n",
    "\n",
    "    vas = vas[vas.name.isin(list(ras[\"name\"].values))].sort_values(\"count\",ascending=False).head(10)\n",
    "\n",
    "    kas = pd.merge(ras, vas, on=\"name\",how=\"left\")\n",
    "\n",
    "    kas.fillna(value=0, inplace=True)\n",
    "    kas.columns = [\"name\",tex1,\"da\",tex2,\"da2\"]\n",
    "    kas.drop([\"da\",\"da2\"],axis=1)\n",
    "    return kas\n",
    "\n",
    "def firstmep(first, second, tex1,tex2):\n",
    "    ras = gobp(first)\n",
    "    ras = ras[~ras[\"entity\"].isnull()]\n",
    "    ras = ras[~ras[\"entity\"].isin([\" \"])]\n",
    "    vas = gobp(second)\n",
    "    vas = vas[~vas[\"entity\"].isnull()]\n",
    "    vas = vas[~vas[\"entity\"].isin([\" \"])]\n",
    "\n",
    "    vas = vas[vas.entity.isin(list(ras[\"entity\"].values))].sort_values(\"count\",ascending=False).head(10)\n",
    "\n",
    "    kas = pd.merge(ras, vas, on=\"entity\",how=\"left\")\n",
    "\n",
    "    kas.fillna(value=0, inplace=True)\n",
    "    kas.columns = [\"entity\",tex1,\"da\",tex2,\"da2\"]\n",
    "    kas.drop([\"da\",\"da2\"],axis=1)\n",
    "    return kas\n",
    "\n",
    "# Normal Nouns Functions\n",
    "kas = firstme(best, worst, \"good\",\"bad\")\n",
    "kas.to_csv(\"good_bad.csv\",index=False)\n",
    "\n",
    "kas = firstme(worst,best, \"bad\",\"good\")\n",
    "kas.to_csv(\"bad_good.csv\",index=False)\n",
    "\n",
    "# Pronoun Functions\n",
    "guud_p = firstmep(best, worst, \"best\",\"worst\")\n",
    "\n",
    "guud_p.to_csv(\"good_bad_pro.csv\",index=False)\n",
    "\n",
    "beet_p = firstmep(worst, best, \"worst\",\"best\")\n",
    "\n",
    "beet_p.to_csv(\"bad_good_pro.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is some boring stuff that does not really lead anywhere. \n",
    "\n",
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame({'token_text':token_text, 'part_of_speech':token_pos})\n",
    "\n",
    "df\n",
    "\n",
    "# Zip is different for Python 3 It is an itterator in three so have to be dealt with.\n",
    "\n",
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape))[:],\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])\n",
    "\n",
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob))[:],\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df\n",
    "\n",
    "# This part of the analysis is different to the on that I am used to. \n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])\n",
    "\n",
    "# Writing all the reviews to a file, each item in the list to a new line\n",
    "# Import os\n",
    "\n",
    "thefile = open('test.txt', 'w')\n",
    "\n",
    "for item in yelp[\"review\"].tolist():\n",
    "  thefile.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "#intermediate_directory = os.path.join('..', 'intermediate')\n",
    "\n",
    "unigram_sentences_filepath = os.path.join('uni_test.txt')\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 0:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(\"test.txt\"):\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "\n",
    "for unigram_sentence in it.islice(unigram_sentences, 10, 20):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')\n",
    "    # Once you have a few more companies the above, bi,tri rams will go down well.\n",
    "\n",
    "# This next one is more interesting, it is topic modelling with LDA:\n",
    "\n",
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material.\n",
    "\n",
    "\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
